{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the XOR function using Tensorflow\n",
    "(using the softplus cost function pg 183)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputing 'x' as the training data and 'y_' as the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([2,5],-5,5))\n",
    "w = tf.Variable(tf.random_uniform([5,1],-5,5))\n",
    "b = tf.Variable(tf.ones([5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden = tf.nn.relu(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = tf.matmul(hidden, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z$ can capture the error better then $y$ after the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softplus = tf.reduce_mean(tf.log(1 + tf.exp((1 - 2 * y_) * z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(softplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = np.array([[0.0,0.0],[0.0,1.0],[1.0,0.0],[1.0,1.0]])\n",
    "training_labels = np.array([[0.0,1.0,1.0,0.0]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6941\n",
      "0.00351988\n",
      "0.00162178\n",
      "0.00100728\n",
      "0.000732415\n",
      "0.000562773\n",
      "0.000490898\n",
      "0.0004063\n",
      "0.000402279\n",
      "0.000269913\n",
      "0.000279714\n",
      "0.000280609\n",
      "0.000233741\n",
      "0.000244588\n",
      "0.000208236\n",
      "0.000191461\n",
      "0.000172421\n",
      "0.000165389\n",
      "0.000137111\n",
      "0.000139316\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    idx = np.random.choice(4, 4)\n",
    "    loss, _ = sess.run([softplus, train_step], feed_dict={x: training_data[idx,:], y_: training_labels[idx,:]})\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(sess.run(y, feed_dict={x:training_data}),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\theta) = \\zeta((1 - 2y)z$$\n",
    "$$ \\zeta = log(1+exp(x))$$\n",
    "Where $z$ is the logit from the hidden layer (pg 183). This is equivaluent to \n",
    "$$ J(\\theta) = y log(\\hat y) + (1 - y) log (1- \\hat y) $$\n",
    "where $\\hat y = \\sigma (z)$\n",
    "\n",
    "Derivation:\n",
    "$$\\zeta((1 - 2y)z$$\n",
    "$$log(1 + exp((1 - 2y)(log(\\hat y) - log(1 - \\hat y))$$\n",
    "$$log(1 + exp((1 - 2y)(log(\\dfrac{\\hat y}{1 - \\hat y})))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = - tf.reduce_mean(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy2 = tf.reduce_mean(tf.log(1 + (tf.exp((1 - 2 * y_) * tf.log(y / (1 - y))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00013134773, 0.00013135975, 0.00013135975]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([cross_entropy, cross_entropy2, softplus], feed_dict={x:training_data, y_:training_labels})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
